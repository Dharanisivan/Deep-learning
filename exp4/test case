from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
import numpy as np

# Sample corpus
corpus = [
    "Deep learning is amazing",
    "Deep learning builds intelligent systems",
    "Intelligent systems can learn"
]

# Tokenize
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1

# Create sequences
sequences = []
for line in corpus:
    seq = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(seq)):
        sequences.append(seq[:i+1])

# Pad sequences
max_len = max(len(x) for x in sequences)
sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')
X, y = sequences[:, :-1], sequences[:, -1]
y = np.eye(total_words)[y]  # one-hot

# Model
model = Sequential([
    Embedding(total_words, 10, input_length=max_len-1),
    LSTM(50),
    Dense(total_words, activation='softmax')
])
model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])
model.fit(X, y, epochs=200, verbose=0)

# Predict function
def predict_next(text):
    seq = tokenizer.texts_to_sequences([text])[0]
    seq = pad_sequences([seq], maxlen=max_len-1, padding='pre')
    pred = np.argmax(model.predict(seq, verbose=0))
    return tokenizer.index_word[pred]

# Test
tests = ["Deep learning is", "Deep learning builds", "Intelligent systems can"]
for t in tests:
    print(f"{t} -> {predict_next(t)}")
output:
Deep learning is -> amazing
Deep learning builds -> intelligent
Intelligent systems can -> learn

